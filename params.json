{"name":"Web crawler","tagline":"A primitive focused web crawler in python","body":"Web_Crawler\r\n===========\r\n\r\nA Web crawler is an Internet bot that systematically browses the World Wide Web, typically for the purpose of Web indexing. A Web crawler may also be called a Web spider, an ant, an automatic indexer, or a Web scutter. Web search engines and some other sites use Web crawling or spidering software to update their web content or indexes of others sites' web content. Web crawlers can copy all the pages they visit for later processing by a search engine that indexes the downloaded pages so that users can search them much more quickly.\r\n\r\nThis web crawler is a focused crawler which takes in a query from the user. It then get the top ten google search results and starts crawling those urls simultaneously using multithreading. For every page that is getting crawled, word occurance count is maintained and all the links are expracted from the page. These extracted links are then recursively crawled based on page relevance (ie. if query string is present on the page). A max Priority Queue is maintained to store the pages (word count as priority) for any future use for page ranking. \r\n\r\nAll the relevant links that are extracted are saved in a file \"links.txt\". The accuracy of crawler is calculated along with the total quantity of data that was downloaded in Mb's. \r\n\r\n------------------------------------------------------------------------\r\n1) Program Structure\r\n\r\nclass Crawler:\r\n\tdef __init__(self, thread_id, url, query, file, lock):\r\n\tdef pagevisited(self, url):\r\n\tdef crawl(self, url):\r\n\tdef run(self):\r\n\r\ndef main():\r\ndef getgoogle(query):\r\n\r\n------------------------------------------------------------------------\r\n2) Input Data\r\n\r\nQuery : Word/words to be searched\r\nPage Limit Number : Number of pages that are to be retireved \r\n\r\nDebug Mode : Prints out the exception messages\r\n\r\n------------------------------------------------------------------------\r\n3) Execution\r\n\r\nStep 1> main -> getgoogle\r\nStep 2> main -> Crawler.start (x10) -> run\r\nStep 3> run  -> crawl\r\nStep 4> crawl-> crawl\r\n\r\nStep 1>\tExecution begins at the main() method which prompts the user \r\n\tfor query input and for number of pages to be found.The query \r\n\tis then passed to getgoogle() method which returns the top 10 \r\n\tsearch results for the given query.\r\n\r\nStep 2>\tThe main() method then iterates over each url returned by \r\n    \tgetgoogle() and creates a thread object Crawler for each url.\r\n    \tEach thread is started and the run() method is invoked for \r\n    \tevery instance of thread.\r\n\r\nStep 3>\tThe run() method then parses the url passed to it and extracts\r\n    \tall links on the page. It then iterates over all the extracted\r\n    \tlinks and calls the crawl() method for crawling each url.\r\n\r\nStep 4>\tThe crawl() method parses the url passed to it. It checks if\r\n    \tthe link has already been visited or not. It then checks the\r\n    \trobots.txt file of the host url and checks if the current url\r\n    \tcan be accessed. Then the word count is calculated and the \r\n    \turl is pushed into the priority queue along with the word count\r\n    \tas page priority. ALso the crawl() method checks if the url\r\n    \tif an anchor jump link. Once the page information is written\r\n    \tonto the file links.txt, all the links on the current page \r\n    \tare extracted. Iterating over these links, each link is\r\n    \tpassed to crawl() method as a recursive call. \r\n\r\nThe program execution continues till the specified number of pages have \r\nbeen found or till the crawler has crawler all the relevant links. Any \r\nexception that is thrown is ignored unless the program is running in \r\ndebug_mode. \r\n\r\nAlso, since there are multiple threads running, we used thread locks\r\nwhile writing data onto the file since we don't want more than one \r\nthread writing onto the file. \r\n\r\n------------------------------------------------------------------------\r\n4) Output Data\r\n\r\nAll the relevant links are saved in a file name links.txt in the same \r\ndirectory. The program also calculates the approximate amount of data \r\nthat was downloaded and what percent of it was relevant.\r\n\r\n------------------------------------------------------------------------\r\n------------------------------------------------------------------------\r\nLibraries Used : \r\nurllib, lxml, heapq, json, math, sys, threading, robotparser\r\n\r\nRequired library installation : \r\nfor lxml use : pip install lxml\r\n\r\nTo Execute :\r\nRun the crawler.py python file. \r\nNo need to create any other file separately. \r\nExtracted links will be saved in links.txt file in same directory.\r\n\r\n------------------------------------------------------------------------\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}